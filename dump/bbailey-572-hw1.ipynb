{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9394a46",
   "metadata": {},
   "source": [
    "572 hw1 - Brad Bailey"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47f9026",
   "metadata": {},
   "source": [
    "Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5934fac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision as tv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762f74a4",
   "metadata": {},
   "source": [
    "Get the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6c9aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_dataset = tv.datasets.CIFAR10(root='./', # here\n",
    "                               train=True, # train split\n",
    "                               download=True, # we want to get the data\n",
    "                               transform=tv.transforms.ToTensor(), # put it into tensor format\n",
    "                          )\n",
    "train_data = torch.utils.data.DataLoader(cifar10_dataset,\n",
    "                        batch_size=2,\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107a770f",
   "metadata": {},
   "source": [
    "Investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21df00d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = iter(train_data) # Let's iterate on it\n",
    "single_point = next(data)\n",
    "print(f\"\"\"Type: {type(single_point)}\n",
    "Length: {len(single_point)}\n",
    "More Types: {type(single_point[0])}, {type(single_point[1])}\n",
    "Shapes: {single_point[0].shape}, {single_point[1].shape}\n",
    "Labels: {single_point[1]}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f00bea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "ToPIL = tv.transforms.ToPILImage() # Converting function\n",
    "img0 = ToPIL(single_point[0][0])\n",
    "img1 = ToPIL(single_point[0][1])\n",
    "# Plotting\n",
    "fig, axs = plt.subplots(1,2)\n",
    "axs[0].imshow(img0)\n",
    "axs[1].imshow(img1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e18f545",
   "metadata": {},
   "source": [
    "Linear Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7378cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class C10Linear(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 ninputs=3*32*32,\n",
    "                 nclasses=10\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(ninputs, nclasses)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1, 3072)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c804a4",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918ae4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from tqdm import tqdm # This is optional but useful\n",
    "\n",
    "# Let's get the right torch device (preference of GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Let's set up some parameters\n",
    "learning_rate=5e-1\n",
    "nepochs = 10\n",
    "ninputs=3*32*32\n",
    "nclasses=10\n",
    "\n",
    "model = C10Linear(ninputs=ninputs,\n",
    "                  nclasses=nclasses).to(device)\n",
    "print(model)\n",
    "# We need an optimizer that tells us what form of gradient descent to do\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# We also need a loss function\n",
    "LossFunction = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# We're going to reload the data here so we have added clarity\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "cifar10_dataset = tv.datasets.CIFAR10(root='./', # here\n",
    "                               train=True, # train split\n",
    "                               download=True, # we want to get the data\n",
    "                               transform=tv.transforms.ToTensor(), # put it into tensor format\n",
    "                        )\n",
    "train_data = torch.utils.data.DataLoader(cifar10_dataset,\n",
    "                        batch_size=batch_size,\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40db409d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.train()\n",
    "loss_history = []\n",
    "loss = torch.Tensor([0])\n",
    "for epoch in tqdm(range(nepochs),\n",
    "                  desc=f\"Epoch\",\n",
    "                  unit=\"epoch\",\n",
    "                  disable=False):\n",
    "    for (data, label) in tqdm(train_data,\n",
    "                              desc=\"iteration\",\n",
    "                              unit=\"%\",\n",
    "                              disable=True):\n",
    "        optimizer.zero_grad(set_to_none=True) # Here we clear the gradients\n",
    "        \n",
    "        # We need to make sure the tensors are on the same device as our model\n",
    "        data = data.to(device)\n",
    "        label = label.to(device)\n",
    "        out = model(data)\n",
    "        \n",
    "        loss = LossFunction(out, label)\n",
    "        \n",
    "        # PyTorch is Magic!\n",
    "        loss.backward() # This function calculates all our gradients\n",
    "        optimizer.step() # This function does our gradient descent with those gradients\n",
    "        loss_history.append(loss.item())\n",
    "    print(f\"Epoch {epoch}: loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1f0520",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Note that we are not plotting loss per epoch but per iteration\n",
    "plt.plot(loss_history)\n",
    "plt.title(\"Neural Network Loss\")\n",
    "plt.xlabel(\"Number of iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a170ea9",
   "metadata": {},
   "source": [
    "Testing testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa193f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_dataset = tv.datasets.CIFAR10(root='./', # here\n",
    "                               train=True, # train split\n",
    "                               download=True, # we want to get the data\n",
    "                               transform=tv.transforms.ToTensor(), # put it into tensor format\n",
    "                        )\n",
    "test_data = torch.utils.data.DataLoader(cifar10_dataset,\n",
    "                        batch_size=batch_size,\n",
    "                        )\n",
    "\n",
    "model.eval()\n",
    "accuracy = 0\n",
    "for (data, label) in test_data:\n",
    "    data = data.to(device)\n",
    "    label = label.to(device)\n",
    "    out = model(data)\n",
    "    answers = out.max(dim=1)[1]\n",
    "    accuracy += (answers == label).sum()\n",
    "print(f\"Total accuracy = {accuracy / len(cifar10_dataset)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce84a65",
   "metadata": {},
   "source": [
    "CNN from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6295b430",
   "metadata": {},
   "outputs": [],
   "source": [
    "class scratchNet(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 ninputs=3*32*32,\n",
    "                 nclasses=10\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.net = torch.nn.Sequential(\n",
    "            \n",
    "        torch.nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.MaxPool2d(2, 2), #64*16*16\n",
    "            \n",
    "        torch.nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.MaxPool2d(2, 2), #128*8*8\n",
    "            \n",
    "        torch.nn.Flatten(),\n",
    "        torch.nn.Linear(128*8*8, 1024), #correct dimensions???\n",
    "        torch.nn.Linear(1024, 512),\n",
    "        torch.nn.Linear(512, 10)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a21e66d",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862c2879",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from tqdm import tqdm # This is optional but useful\n",
    "\n",
    "# Let's get the right torch device (preference of GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Let's set up some parameters\n",
    "learning_rate=1\n",
    "nepochs = 10\n",
    "ninputs=3*32*32\n",
    "nclasses=10\n",
    "\n",
    "model = scratchNet(ninputs=ninputs,\n",
    "                  nclasses=nclasses).to(device)\n",
    "print(model)\n",
    "# We need an optimizer that tells us what form of gradient descent to do\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# We also need a loss function\n",
    "LossFunction = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# We're going to reload the data here so we have added clarity\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "cifar10_dataset = tv.datasets.CIFAR10(root='./', # here\n",
    "                               train=True, # train split\n",
    "                               download=True, # we want to get the data\n",
    "                               transform=tv.transforms.ToTensor(), # put it into tensor format\n",
    "                        )\n",
    "train_data = torch.utils.data.DataLoader(cifar10_dataset,\n",
    "                        batch_size=batch_size,\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e865c98",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.train()\n",
    "loss_history = []\n",
    "loss = torch.Tensor([0])\n",
    "for epoch in tqdm(range(nepochs),\n",
    "                  desc=f\"Epoch\",\n",
    "                  unit=\"epoch\",\n",
    "                  disable=False):\n",
    "    for (data, label) in tqdm(train_data,\n",
    "                              desc=\"iteration\",\n",
    "                              unit=\"%\",\n",
    "                              disable=True):\n",
    "        optimizer.zero_grad(set_to_none=True) # Here we clear the gradients\n",
    "        \n",
    "        # We need to make sure the tensors are on the same device as our model\n",
    "        data = data.to(device)\n",
    "        label = label.to(device)\n",
    "        out = model(data)\n",
    "        \n",
    "        loss = LossFunction(out, label)\n",
    "        \n",
    "        # PyTorch is Magic!\n",
    "        loss.backward() # This function calculates all our gradients\n",
    "        optimizer.step() # This function does our gradient descent with those gradients\n",
    "        loss_history.append(loss.item())\n",
    "    print(f\"Epoch {epoch}: loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19700994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that we are not plotting loss per epoch but per iteration\n",
    "plt.plot(loss_history)\n",
    "plt.title(\"Neural Network Loss\")\n",
    "plt.xlabel(\"Number of iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f591b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_dataset = tv.datasets.CIFAR10(root='./', # here\n",
    "                               train=True, # train split\n",
    "                               download=True, # we want to get the data\n",
    "                               transform=tv.transforms.ToTensor(), # put it into tensor format\n",
    "                        )\n",
    "test_data = torch.utils.data.DataLoader(cifar10_dataset,\n",
    "                        batch_size=batch_size,\n",
    "                        )\n",
    "\n",
    "model.eval()\n",
    "accuracy = 0\n",
    "for (data, label) in test_data:\n",
    "    data = data.to(device)\n",
    "    label = label.to(device)\n",
    "    out = model(data)\n",
    "    answers = out.max(dim=1)[1]\n",
    "    accuracy += (answers == label).sum()\n",
    "print(f\"Total accuracy = {accuracy / len(cifar10_dataset)*100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

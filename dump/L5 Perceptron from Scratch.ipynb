{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53d3e68d-b968-4084-ba36-47bfe4506bcf",
   "metadata": {},
   "source": [
    "# ML from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55b954b-d299-4eaa-8313-f885cd660c1a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a28da8b-7427-46aa-9650-bf6a30cd78c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credit Card Application Example\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "### ------- How INPUT FEATURES X are generated --------\n",
    "\n",
    "def len_gen(mean, var, n=1):\n",
    "    x = np.random.normal(mean, var, n).astype(int)\n",
    "    x = np.clip(x, 0, 98)\n",
    "    print('Length of credit history (month): ', x)\n",
    "    return x\n",
    "\n",
    "def debt_gen(mean, var, n=1):\n",
    "    x = np.random.normal(mean, var, n).astype(int)\n",
    "    print('Debt owed (USD): ', x)\n",
    "    return x\n",
    "\n",
    "def num_gen(mean, var, n=1):\n",
    "    x = np.random.normal(mean, var, n).astype(int)\n",
    "    x = np.clip(x, 0, 50)\n",
    "    print('Number of credit cards: ', x)\n",
    "    return x\n",
    "\n",
    "### ------- How INPUT Labels Y are generated --------\n",
    "\n",
    "def score_func_1(leng):\n",
    "    s = 0\n",
    "    if leng < 18:\n",
    "        s = leng\n",
    "    else:\n",
    "        s = 50 + 0.625 * (leng-18)\n",
    "    return s\n",
    "\n",
    "def score_func_2(debt):\n",
    "    s = 100/(1+np.exp((debt-30000)/10000))\n",
    "    return s\n",
    "\n",
    "def score_func_3(num):\n",
    "    if num <= 10 and num >= 4:\n",
    "        s = 100\n",
    "    elif num > 10:\n",
    "        s = 100 - (num-10)*10\n",
    "    elif num < 4:\n",
    "        s = 25*num\n",
    "    return s\n",
    "\n",
    "def decision(factor_list, score_func_list=[score_func_1, score_func_2, score_func_3], \n",
    "             score_weight_list=(0.2,0.6,0.4), threshold=80):\n",
    "    score = 0\n",
    "    for i in range(len(factor_list)):\n",
    "        score += np.array(list(map(score_func_list[i], factor_list[i])))*score_weight_list[i]\n",
    "    # y = np.sign(score-threshold)\n",
    "    y = np.sign(score-threshold + np.random.normal()*50)\n",
    "    print('Decisions: ',y)\n",
    "    print('Approval rate: ', sum(y==1)/len(y))\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13120fff-fea0-4eef-be92-01e6ab751893",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Observed some data\n",
    "n = 50 # number of samples\n",
    "x1 = len_gen(24, 12, n)\n",
    "x2 = debt_gen(30000, 40000, n)\n",
    "x3 = num_gen(5, 10, n)\n",
    "y = decision([x1,x2,x3], threshold=60)\n",
    "\n",
    "### Some test data\n",
    "n = 100 # number of samples\n",
    "x1_test = len_gen(24, 12, n)\n",
    "x2_test = debt_gen(30000, 40000, n)\n",
    "x3_test = num_gen(5, 10, n)\n",
    "y = decision([x1,x2,x3], threshold=60)\n",
    "\n",
    "### Some test data (Out-of-distribution)\n",
    "n = 100 # number of samples\n",
    "x1_test_ood = len_gen(24, 12, n)\n",
    "x2_test_ood = debt_gen(30000, 40000, n)\n",
    "x3_test_ood = num_gen(5, 10, n)\n",
    "y = decision([x1,x2,x3], threshold=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43b2209-5971-4d9d-b5a0-26f7f28c0995",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61db735f-6506-409e-b32c-d55a82eaf0df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a36e8eb-82eb-4e2b-ac1e-c8e3ff930f84",
   "metadata": {},
   "source": [
    "## Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10361bc-7254-4000-8f6d-917c40b4228f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47ef9288-eb84-4882-8987-b9536ae2be23",
   "metadata": {},
   "source": [
    "## The Perceptron Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab6f301-0dea-409c-bf55-48767ff0c5f2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7361dd-d14c-46d0-b33a-3d6c5387a2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class perceptron:\n",
    "    def __init__(self):\n",
    "        self.w1, self.w2, self.w3 = np.random.randn(3)\n",
    "        self.t = np.random.randint(40)\n",
    "        print('INITIALIZATION:')\n",
    "        print('weights & threshold initialized: ', self.w1, self.w2, self.w3, self.t)\n",
    "        print(''.center(100,'-'))\n",
    "        \n",
    "    def inference(self, x1, x2, x3):\n",
    "        y_pred = np.sign(self.w1*x1 + self.w2*x2 + self.w3*x3 - self.t)\n",
    "        print('INFERENCE:')\n",
    "        print('current model weights & threshold: ', self.w1, self.w2, self.w3, self.t)\n",
    "        print('predicted results', y)\n",
    "        print('Approval rate (predicted): ', sum(y_pred==1)/len(y_pred))\n",
    "        print(''.center(100,'-'))\n",
    "        return y_pred\n",
    "\n",
    "    def evaluate(self, x1, x2, x3, y):\n",
    "        y_pred = np.sign(self.w1*x1 + self.w2*x2 + self.w3*x3 - self.t)\n",
    "        acc = sum(y==y_pred)/len(y)\n",
    "        print('EVALUATION:')\n",
    "        print('current model weights & threshold: ', self.w1, self.w2, self.w3, self.t)\n",
    "        print('Predicted results', y_pred)\n",
    "        print('Ground truth results', y)\n",
    "        print('Approval rate (ground truth): ', sum(y==1)/len(y))\n",
    "        print('Approval rate (predicted): ', sum(y_pred==1)/len(y_pred))\n",
    "        print('Accuracy: ', acc)\n",
    "        print(''.center(100,'-'))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2d946a-fe84-44c6-b4b9-1442c99fd41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = perceptron()\n",
    "y_pred = model.inference(x1,x2,x3)\n",
    "model.evaluate(x1,x2,x3,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ebf449-1486-4de1-bc1d-acf725b8d3e5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b802afd-90b9-4c23-8c46-139ed216f0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class perceptron:\n",
    "    def __init__(self):\n",
    "        self.w1, self.w2, self.w3 = np.random.randn(3)\n",
    "        self.t = np.random.randint(40)\n",
    "        print('INITIALIZATION:')\n",
    "        print('weights & threshold initialized: ', self.w1, self.w2, self.w3, self.t)\n",
    "        print(''.center(100,'-'))\n",
    "        \n",
    "    def inference(self, x1, x2, x3):\n",
    "        y_pred = np.sign(self.w1*x1 + self.w2*x2 + self.w3*x3 - self.t)\n",
    "        print('INFERENCE:')\n",
    "        print('current model weights & threshold: ', self.w1, self.w2, self.w3, self.t)\n",
    "        print('predicted results', y)\n",
    "        print('Approval rate (predicted): ', sum(y_pred==1)/len(y_pred))\n",
    "        print(''.center(100,'-'))\n",
    "        return y_pred\n",
    "\n",
    "    def evaluate(self, x1, x2, x3, y):\n",
    "        y_pred = np.sign(self.w1*x1 + self.w2*x2 + self.w3*x3 - self.t)\n",
    "        acc = sum(y==y_pred)/len(y)\n",
    "        print('EVALUATION:')\n",
    "        print('current model weights & threshold: ', self.w1, self.w2, self.w3, self.t)\n",
    "        print('Predicted results', y_pred)\n",
    "        print('Ground truth results', y)\n",
    "        print('Approval rate (ground truth): ', sum(y==1)/len(y))\n",
    "        print('Approval rate (predicted): ', sum(y_pred==1)/len(y_pred))\n",
    "        print('Accuracy: ', acc)\n",
    "        print(''.center(100,'-'))\n",
    "    \n",
    "    def learn(self, x1, x2, x3, y):\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2aad9a-42ca-442d-bcd2-e9b6792bcc28",
   "metadata": {},
   "source": [
    "### Perceptron Learning Algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf1e7be-e37f-4177-9df4-f522ad793fa1",
   "metadata": {},
   "source": [
    "### PLA\n",
    "For $t = 0, 1, …$\n",
    "\n",
    " Find a mistake of $𝑤_𝑡$ at $(𝑋_𝑛, 𝑦_𝑛)$, Where $𝑔_𝑡 (𝑋_𝑛 )=𝑠𝑖𝑔𝑛(𝑊_𝑡^𝑇 𝑋_𝑛)≠𝑦_𝑛$\n",
    " \n",
    "Try to correct the mistake by:\n",
    "$W_{t+1}$ = $𝑊_𝑡$+$𝑦_𝑛$$𝑋_𝑛$\n",
    "    \n",
    "… until no more mistakes\n",
    "Return the last $W$ as $g$. (written as $𝑊_{𝑃𝐿𝐴}$)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Try to work on this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4785ed-11b8-485e-a58b-9af347f633b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class perceptron:\n",
    "    def __init__(self):\n",
    "        self.w1, self.w2, self.w3 = np.random.randn(3)\n",
    "        self.t = np.random.randint(40)\n",
    "        print('INITIALIZATION:')\n",
    "        print('weights & threshold initialized: ', self.w1, self.w2, self.w3, self.t)\n",
    "        print(''.center(100,'-'))\n",
    "        \n",
    "    def inference(self, x1, x2, x3, print_results=False):\n",
    "        y_pred = np.sign(self.w1*x1 + self.w2*x2 + self.w3*x3 - self.t)\n",
    "        if print_results == True:\n",
    "            print('INFERENCE:')\n",
    "            print('current model weights & threshold: ', self.w1, self.w2, self.w3, self.t)\n",
    "            print('predicted results', y)\n",
    "            # print('Approval rate (predicted): ', sum(y_pred==1)/len(y_pred))\n",
    "            print(''.center(100,'-'))\n",
    "        return y_pred\n",
    "\n",
    "    def evaluate(self, x1, x2, x3, y):\n",
    "        y_pred = np.sign(self.w1*x1 + self.w2*x2 + self.w3*x3 - self.t)\n",
    "        acc = sum(y==y_pred)/len(y)\n",
    "        print('EVALUATION:')\n",
    "        print('current model weights & threshold: ', self.w1, self.w2, self.w3, self.t)\n",
    "        print('Predicted results', y_pred)\n",
    "        print('Ground truth results', y)\n",
    "        print('Approval rate (ground truth): ', sum(y==1)/len(y))\n",
    "        print('Approval rate (predicted): ', sum(y_pred==1)/len(y_pred))\n",
    "        print('Accuracy: ', acc)\n",
    "        print(''.center(100,'-'))\n",
    "    \n",
    "    def learn(self, x1, x2, x3, y, print_acc=False):\n",
    "        N = len(x1)\n",
    "        assert len(x1)==len(x2)==len(x3)==len(y)\n",
    "        for i in range(N):\n",
    "            if self.inference(x1[i], x2[i], x3[i]) != y[i]:\n",
    "                print('before update:', self.w1, self.w2, self.w3, self.t)\n",
    "                self.w1 = self.w1 + y[i]*x1[i]\n",
    "                self.w2 = self.w2 + y[i]*x2[i]\n",
    "                self.w3 = self.w3 + y[i]*x3[i]\n",
    "                self.t = self.t   + y[i]*(-1)\n",
    "                print('after update:', self.w1, self.w2, self.w3, self.t)\n",
    "                if print_acc:\n",
    "                    y_pred = np.sign(self.w1*x1 + self.w2*x2 + self.w3*x3 - self.t)\n",
    "                    acc = sum(y==y_pred)/len(y)\n",
    "                    print('--->Accuracy: ', acc)\n",
    "                    \n",
    "            else:\n",
    "                print('skip')\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a61aba8-1e3c-4bc9-acd7-80ed1402cc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = perceptron()\n",
    "y_pred = model.inference(x1,x2,x3,True)\n",
    "model.evaluate(x1,x2,x3,y)\n",
    "model.learn(x1,x2,x3,y, True)\n",
    "model.evaluate(x1,x2,x3,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6e398e-de8b-45ac-bcb5-117073e454a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DONE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9e176c-4788-4c5d-b2a9-460a5040f177",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Cyclic PLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a574db7a-4e68-4c18-8d8d-c1df2657372e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Pocket PLA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d52e262-9a7d-44ca-bf07-738c907965fa",
   "metadata": {},
   "source": [
    "### Learning with visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7e92f9-981a-4eea-97b8-6c51d8234c63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c4d8583-3a11-4c28-9501-179cbc13d06e",
   "metadata": {},
   "source": [
    "### Learning with visualization & loss curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3987dc-fb4d-4f8e-9bee-b75b3b36aed2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1c362d87-c931-4cbd-99ac-1d11882519ea",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76efaebe-6d7b-4fe4-8eb2-f37fd0cef698",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1fd7afcb-5cc1-4f2f-bfb4-4a32ced6849d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2592a1-c3ec-4e4f-867f-450d898da3ab",
   "metadata": {},
   "source": [
    "### MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddfe3e3-363d-48ca-9b06-f03a44b322a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ebf2424-869c-485a-acd8-18b396175331",
   "metadata": {},
   "source": [
    "### Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d188e1-b43f-4918-a70b-706fe2c2f8e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca245430",
   "metadata": {},
   "outputs": [],
   "source": [
    "from posixpath import split\n",
    "import torch\n",
    "import time\n",
    "from torch import nn\n",
    "from torchtext.datasets import IMDB\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f86b165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('neg',\n",
       " \"If only to avoid making this type of film in the future. This film is interesting as an experiment but tells no cogent story.<br /><br />One might feel virtuous for sitting thru it because it touches on so many IMPORTANT issues but it does so without any discernable motive. The viewer comes away with no new perspectives (unless one comes up with one while one's mind wanders, as it will invariably do during this pointless film).<br /><br />One might better spend one's time staring out a window at a tree growing.<br /><br />\")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_iter = IMDB(split='train')\n",
    "next(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67c73653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('neg',\n",
       " \"This film was probably inspired by Godard's Masculin, f√©minin and I urge you to see that film instead.<br /><br />The film has two strong elements and those are, (1) the realistic acting (2) the impressive, undeservedly good, photo. Apart from that, what strikes me most is the endless stream of silliness. Lena Nyman has to be most annoying actress in the world. She acts so stupid and with all the nudity in this film,...it's unattractive. Comparing to Godard's film, intellectuality has been replaced with stupidity. Without going too far on this subject, I would say that follows from the difference in ideals between the French and the Swedish society.<br /><br />A movie of its time, and place. 2/10.\")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad46bd0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "test = ['pos', 'neg']\n",
    "\n",
    "def label_pipeline(x):\n",
    "    rating = {'pos': 0, 'neg': 1}\n",
    "    return rating[x]\n",
    "\n",
    "print(label_pipeline(test[0]))\n",
    "print(label_pipeline(test[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7cc49a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "| epoch   1 |   500/  743 batches | accuracy    0.656\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time: 42.62s | valid accuracy    0.802 \n",
      "-----------------------------------------------------------\n",
      "| epoch   2 |   500/  743 batches | accuracy    0.802\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time: 42.26s | valid accuracy    0.760 \n",
      "-----------------------------------------------------------\n",
      "| epoch   3 |   500/  743 batches | accuracy    0.861\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time: 42.53s | valid accuracy    0.845 \n",
      "-----------------------------------------------------------\n",
      "| epoch   4 |   500/  743 batches | accuracy    0.863\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   4 | time: 42.10s | valid accuracy    0.850 \n",
      "-----------------------------------------------------------\n",
      "| epoch   5 |   500/  743 batches | accuracy    0.869\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   5 | time: 40.76s | valid accuracy    0.844 \n",
      "-----------------------------------------------------------\n",
      "| epoch   6 |   500/  743 batches | accuracy    0.873\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   6 | time: 41.43s | valid accuracy    0.857 \n",
      "-----------------------------------------------------------\n",
      "| epoch   7 |   500/  743 batches | accuracy    0.872\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   7 | time: 41.37s | valid accuracy    0.856 \n",
      "-----------------------------------------------------------\n",
      "| epoch   8 |   500/  743 batches | accuracy    0.874\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   8 | time: 41.49s | valid accuracy    0.857 \n",
      "-----------------------------------------------------------\n",
      "| epoch   9 |   500/  743 batches | accuracy    0.873\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   9 | time: 41.36s | valid accuracy    0.857 \n",
      "-----------------------------------------------------------\n",
      "| epoch  10 |   500/  743 batches | accuracy    0.871\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  10 | time: 40.04s | valid accuracy    0.857 \n",
      "-----------------------------------------------------------\n",
      "Checking the results of test dataset.\n",
      "test accuracy    0.856\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "EPOCHS = 10 \n",
    "LR = 5 \n",
    "BATCH_SIZE = 32\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "train_iter = IMDB(split='train')\n",
    "\n",
    "#pipeline: tokenize, build , and collate\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "# this line doesnt't work for IMDB\n",
    "# label_pipeline = lambda x: int(x) - 1\n",
    "#try w/o -1???\n",
    "#label_pipeline = lambda x: int(x)\n",
    "\n",
    "def label_pipeline(x):\n",
    "    rating = {'pos': 0, 'neg': 1} ## need to be careful/consistent here\n",
    "    return rating[x]\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "    for (_label, _text) in batch:\n",
    "         label_list.append(label_pipeline(_label))\n",
    "         processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "         text_list.append(processed_text)\n",
    "         offsets.append(processed_text.size(0))\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text_list = torch.cat(text_list)\n",
    "    return label_list.to(device), text_list.to(device), offsets.to(device)\n",
    "\n",
    "class TextClassificationModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)\n",
    "\n",
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 500\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predicted_label = model(text, offsets)\n",
    "        loss = criterion(predicted_label, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
    "                  '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n",
    "                                              total_acc/total_count))\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "            predicted_label = model(text, offsets)\n",
    "            loss = criterion(predicted_label, label)\n",
    "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc/total_count\n",
    "\n",
    "### DATA LOADERS ###\n",
    "\n",
    "train_iter, test_iter = IMDB()\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "test_dataset = to_map_style_dataset(test_iter)\n",
    "num_train = int(len(train_dataset) * 0.95)\n",
    "split_train_, split_valid_ = \\\n",
    "    random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
    "\n",
    "train_dataloader = DataLoader(split_train_, batch_size=BATCH_SIZE,\n",
    "                              shuffle=True, collate_fn=collate_batch)\n",
    "valid_dataloader = DataLoader(split_valid_, batch_size=BATCH_SIZE,\n",
    "                              shuffle=True, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
    "                             shuffle=True, collate_fn=collate_batch)\n",
    "\n",
    "\n",
    "### MODEL SETUP ###\n",
    "\n",
    "#num_class = len(set([label for (label, text) in train_iter]))\n",
    "#sanity - should be 2 - doesn't work the pytorch way\n",
    "num_class = 2\n",
    "print(num_class)\n",
    "vocab_size = len(vocab)\n",
    "emsize = 64\n",
    "model = TextClassificationModel(vocab_size, emsize, num_class).to(device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "total_accu = None\n",
    "\n",
    "### TRAIN AND TEST ###\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(train_dataloader)\n",
    "    accu_val = evaluate(valid_dataloader)\n",
    "    if total_accu is not None and total_accu > accu_val:\n",
    "      scheduler.step()\n",
    "    else:\n",
    "       total_accu = accu_val\n",
    "    print('-' * 59)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
    "          'valid accuracy {:8.3f} '.format(epoch,\n",
    "                                           time.time() - epoch_start_time,\n",
    "                                           accu_val))\n",
    "    print('-' * 59)\n",
    "\n",
    "print('Checking the results of test dataset.')\n",
    "accu_test = evaluate(test_dataloader)\n",
    "print('test accuracy {:8.3f}'.format(accu_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5defea4f",
   "metadata": {},
   "source": [
    "### AG NEWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "244e4c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM CLASSES\n",
      "4\n",
      "| epoch   1 |   500/ 1782 batches | accuracy    0.691\n",
      "| epoch   1 |  1000/ 1782 batches | accuracy    0.857\n",
      "| epoch   1 |  1500/ 1782 batches | accuracy    0.876\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time: 47.15s | valid accuracy    0.874 \n",
      "-----------------------------------------------------------\n",
      "| epoch   2 |   500/ 1782 batches | accuracy    0.894\n",
      "| epoch   2 |  1000/ 1782 batches | accuracy    0.903\n",
      "| epoch   2 |  1500/ 1782 batches | accuracy    0.904\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time: 47.04s | valid accuracy    0.893 \n",
      "-----------------------------------------------------------\n",
      "| epoch   3 |   500/ 1782 batches | accuracy    0.913\n",
      "| epoch   3 |  1000/ 1782 batches | accuracy    0.917\n",
      "| epoch   3 |  1500/ 1782 batches | accuracy    0.914\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time: 46.71s | valid accuracy    0.904 \n",
      "-----------------------------------------------------------\n",
      "| epoch   4 |   500/ 1782 batches | accuracy    0.924\n",
      "| epoch   4 |  1000/ 1782 batches | accuracy    0.924\n",
      "| epoch   4 |  1500/ 1782 batches | accuracy    0.924\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   4 | time: 48.26s | valid accuracy    0.906 \n",
      "-----------------------------------------------------------\n",
      "| epoch   5 |   500/ 1782 batches | accuracy    0.934\n",
      "| epoch   5 |  1000/ 1782 batches | accuracy    0.928\n",
      "| epoch   5 |  1500/ 1782 batches | accuracy    0.926\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   5 | time: 48.50s | valid accuracy    0.905 \n",
      "-----------------------------------------------------------\n",
      "| epoch   6 |   500/ 1782 batches | accuracy    0.943\n",
      "| epoch   6 |  1000/ 1782 batches | accuracy    0.943\n",
      "| epoch   6 |  1500/ 1782 batches | accuracy    0.941\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   6 | time: 48.22s | valid accuracy    0.908 \n",
      "-----------------------------------------------------------\n",
      "| epoch   7 |   500/ 1782 batches | accuracy    0.943\n",
      "| epoch   7 |  1000/ 1782 batches | accuracy    0.944\n",
      "| epoch   7 |  1500/ 1782 batches | accuracy    0.946\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   7 | time: 48.09s | valid accuracy    0.909 \n",
      "-----------------------------------------------------------\n",
      "| epoch   8 |   500/ 1782 batches | accuracy    0.946\n",
      "| epoch   8 |  1000/ 1782 batches | accuracy    0.946\n",
      "| epoch   8 |  1500/ 1782 batches | accuracy    0.944\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   8 | time: 48.32s | valid accuracy    0.907 \n",
      "-----------------------------------------------------------\n",
      "| epoch   9 |   500/ 1782 batches | accuracy    0.946\n",
      "| epoch   9 |  1000/ 1782 batches | accuracy    0.947\n",
      "| epoch   9 |  1500/ 1782 batches | accuracy    0.948\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   9 | time: 47.91s | valid accuracy    0.909 \n",
      "-----------------------------------------------------------\n",
      "| epoch  10 |   500/ 1782 batches | accuracy    0.947\n",
      "| epoch  10 |  1000/ 1782 batches | accuracy    0.948\n",
      "| epoch  10 |  1500/ 1782 batches | accuracy    0.947\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  10 | time: 47.06s | valid accuracy    0.909 \n",
      "-----------------------------------------------------------\n",
      "Checking the results of test dataset.\n",
      "test accuracy    0.911\n"
     ]
    }
   ],
   "source": [
    "#SOURCE: https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html\n",
    "# import torch\n",
    "from torchtext.datasets import AG_NEWS\n",
    "train_iter = AG_NEWS(split='train')\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "train_iter = AG_NEWS(split='train')\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "label_pipeline = lambda x: int(x) - 1\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "    for (_label, _text) in batch:\n",
    "         label_list.append(label_pipeline(_label))\n",
    "         processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "         text_list.append(processed_text)\n",
    "         offsets.append(processed_text.size(0))\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text_list = torch.cat(text_list)\n",
    "    return label_list.to(device), text_list.to(device), offsets.to(device)\n",
    "\n",
    "train_iter = AG_NEWS(split='train')\n",
    "dataloader = DataLoader(train_iter, batch_size=8, shuffle=False, collate_fn=collate_batch)\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "class TextClassificationModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)\n",
    "\n",
    "train_iter = AG_NEWS(split='train')\n",
    "num_class = len(set([label for (label, text) in train_iter]))\n",
    "print(\"NUM CLASSES\")\n",
    "print(num_class)\n",
    "vocab_size = len(vocab)\n",
    "emsize = 64\n",
    "model = TextClassificationModel(vocab_size, emsize, num_class).to(device)\n",
    "\n",
    "import time\n",
    "\n",
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 500\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predicted_label = model(text, offsets)\n",
    "        loss = criterion(predicted_label, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
    "                  '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n",
    "                                              total_acc/total_count))\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "            predicted_label = model(text, offsets)\n",
    "            loss = criterion(predicted_label, label)\n",
    "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc/total_count\n",
    "\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "# Hyperparameters\n",
    "EPOCHS = 10 # epoch\n",
    "LR = 5  # learning rate\n",
    "BATCH_SIZE = 64 # batch size for training\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "total_accu = None\n",
    "train_iter, test_iter = AG_NEWS()\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "test_dataset = to_map_style_dataset(test_iter)\n",
    "num_train = int(len(train_dataset) * 0.95)\n",
    "split_train_, split_valid_ = \\\n",
    "    random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
    "\n",
    "train_dataloader = DataLoader(split_train_, batch_size=BATCH_SIZE,\n",
    "                              shuffle=True, collate_fn=collate_batch)\n",
    "valid_dataloader = DataLoader(split_valid_, batch_size=BATCH_SIZE,\n",
    "                              shuffle=True, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
    "                             shuffle=True, collate_fn=collate_batch)\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(train_dataloader)\n",
    "    accu_val = evaluate(valid_dataloader)\n",
    "    if total_accu is not None and total_accu > accu_val:\n",
    "      scheduler.step()\n",
    "    else:\n",
    "       total_accu = accu_val\n",
    "    print('-' * 59)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
    "          'valid accuracy {:8.3f} '.format(epoch,\n",
    "                                           time.time() - epoch_start_time,\n",
    "                                           accu_val))\n",
    "    print('-' * 59)\n",
    "\n",
    "print('Checking the results of test dataset.')\n",
    "accu_test = evaluate(test_dataloader)\n",
    "print('test accuracy {:8.3f}'.format(accu_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
